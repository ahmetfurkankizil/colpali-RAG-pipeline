{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6764ba45",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Install dependencies ---\n",
    "!pip install \"colpali-engine>=0.3.0,<0.4.0\" pymupdf pillow transformers==4.51.1 openai gradio --quiet\n",
    "\n",
    "# --- Imports ---\n",
    "import fitz\n",
    "import torch\n",
    "import requests\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "from colpali_engine.models import ColPali, ColPaliProcessor\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from io import BytesIO\n",
    "import traceback\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# --- Setup ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Load Models ---\n",
    "model_name = \"vidore/colpali-v1.2\"\n",
    "model = ColPali.from_pretrained(model_name, torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32, device_map={\"\": device} if device == \"cuda\" else None).eval()\n",
    "processor = ColPaliProcessor.from_pretrained(model_name)\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def normalize(v):\n",
    "    norm = np.linalg.norm(v, axis=-1, keepdims=True)\n",
    "    return v / np.clip(norm, 1e-6, None)\n",
    "\n",
    "def pad_embedding(emb, target_len=32):\n",
    "    emb = normalize(emb[:target_len])\n",
    "    if emb.shape[0] < target_len:\n",
    "        pad = np.zeros((target_len - emb.shape[0], emb.shape[1]), dtype=np.float32)\n",
    "        emb = np.concatenate([emb, pad], axis=0)\n",
    "    return emb\n",
    "\n",
    "def extract_chunks_and_images(pdf_path, chunk_size=500):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text_chunks = []\n",
    "    page_images = []\n",
    "    for page_num, page in enumerate(doc):\n",
    "        text = page.get_text(\"text\")\n",
    "        for i in range(0, len(text), chunk_size):\n",
    "            chunk = text[i:i+chunk_size].strip()\n",
    "            if chunk:\n",
    "                text_chunks.append({\"page\": page_num, \"text\": chunk})\n",
    "        pix = page.get_pixmap(dpi=150)\n",
    "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "        page_images.append(img)\n",
    "    return text_chunks, page_images\n",
    "\n",
    "def generate_image_captions(images):\n",
    "    captions = []\n",
    "    for page_num, img in enumerate(images):\n",
    "        inputs = blip_processor(img, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            out = blip_model.generate(**inputs)\n",
    "        caption = blip_processor.decode(out[0], skip_special_tokens=True)\n",
    "        captions.append({\"page\": page_num, \"caption\": caption})\n",
    "    return captions\n",
    "\n",
    "def embed_and_push_to_vespa(text_chunks, images, captions, endpoint):\n",
    "    print(f\"[INFO] Embedding and pushing {len(text_chunks)} text chunks and {len(images)} images to Vespa...\")\n",
    "    docs = []\n",
    "    try:\n",
    "        texts = [c[\"text\"] for c in text_chunks]\n",
    "        text_inputs = processor.process_queries(texts)\n",
    "        text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            text_embs = model(**text_inputs)\n",
    "\n",
    "        for i, (chunk, emb) in enumerate(zip(text_chunks, text_embs)):\n",
    "            emb_np = emb.to(torch.float32).cpu().numpy()\n",
    "            multi_vec = pad_embedding(emb_np, 32)\n",
    "            doc = {\n",
    "                \"put\": f\"id:multimodal-content:multimodal_doc::{i}\",\n",
    "                \"fields\": {\n",
    "                    \"title\": f\"Text chunk page {chunk['page']}\",\n",
    "                    \"description\": chunk[\"text\"][:200],\n",
    "                    \"page\": chunk[\"page\"],\n",
    "                    \"modality\": \"text\",\n",
    "                    \"embedding\": {\"values\": multi_vec.tolist()}\n",
    "                }\n",
    "            }\n",
    "            docs.append({\"embedding\": multi_vec, \"desc\": chunk[\"text\"][:200], \"page\": chunk[\"page\"], \"modality\": \"text\"})\n",
    "            r = requests.post(f\"{endpoint}/document/v1/multimodal-content/multimodal_doc/docid/{i}\", json=doc)\n",
    "            print(f\"Text doc {i} push: {r.status_code}\")\n",
    "\n",
    "        image_inputs = processor.process_images(images)\n",
    "        image_inputs = {k: v.to(device) for k, v in image_inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            image_embs = model(**image_inputs)\n",
    "\n",
    "        for i, (cap, emb) in enumerate(zip(captions, image_embs)):\n",
    "            emb_np = emb.to(torch.float32).cpu().numpy()\n",
    "            multi_vec = pad_embedding(emb_np, 32)\n",
    "            doc_id = f\"img_{i}\"\n",
    "            doc = {\n",
    "                \"put\": f\"id:multimodal-content:multimodal_doc::{doc_id}\",\n",
    "                \"fields\": {\n",
    "                    \"title\": f\"Image page {cap['page']}\",\n",
    "                    \"description\": cap[\"caption\"],\n",
    "                    \"page\": cap[\"page\"],\n",
    "                    \"modality\": \"image\",\n",
    "                    \"embedding\": {\"values\": multi_vec.tolist()}\n",
    "                }\n",
    "            }\n",
    "            docs.append({\"embedding\": multi_vec, \"desc\": cap[\"caption\"], \"page\": cap[\"page\"], \"modality\": \"image\"})\n",
    "            r = requests.post(f\"{endpoint}/document/v1/multimodal-content/multimodal_doc/docid/{doc_id}\", json=doc)\n",
    "            print(f\"Image doc {doc_id} push: {r.status_code}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR] Vespa push failed:\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    return docs\n",
    "\n",
    "def encode_query(text):\n",
    "    inputs = processor.process_queries([text])\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        emb = model(**inputs)[0].to(torch.float32).cpu().numpy()\n",
    "    return pad_embedding(emb, 32)\n",
    "\n",
    "def maxsim_score(query_emb, doc_emb):\n",
    "    return float(np.sum(np.max(np.matmul(query_emb, doc_emb.T), axis=1)))\n",
    "\n",
    "def maxsim_search(query_text, docs, top_k_text=3, top_k_img=3):\n",
    "    query_emb = encode_query(query_text)\n",
    "    text_hits = []\n",
    "    image_hits = []\n",
    "    for doc in docs:\n",
    "        score = maxsim_score(query_emb, doc[\"embedding\"])\n",
    "        if doc[\"modality\"] == \"text\":\n",
    "            text_hits.append((score, doc[\"desc\"], doc[\"page\"]))\n",
    "        elif doc[\"modality\"] == \"image\":\n",
    "            image_hits.append((score, doc[\"desc\"], doc[\"page\"]))\n",
    "    text_hits.sort(key=lambda x: -x[0])\n",
    "    image_hits.sort(key=lambda x: -x[0])\n",
    "    return text_hits[:top_k_text], image_hits[:top_k_img]\n",
    "\n",
    "def save_for_generation(query, text_hits, image_hits):\n",
    "    results = {\n",
    "        \"query\": query,\n",
    "        \"top_texts\": text_hits,\n",
    "        \"top_images\": [{\"page\": page, \"caption\": desc} for _, desc, page in image_hits]\n",
    "    }\n",
    "\n",
    "    with open(\"retrieved_context.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    for _, _, page in image_hits[:3]:\n",
    "        if page in image_cache:\n",
    "            image_cache[page].save(f\"page_{page}.png\")\n",
    "\n",
    "    print(\"[INFO] Saved retrieved_context.json and corresponding images as page_<number>.png\")\n",
    "\n",
    "vespa_endpoint = \"https://146e-2a02-ff0-c06-1e42-c845-7837-93e2-cfab.ngrok-free.app\"\n",
    "image_cache = {}\n",
    "embedded_docs = []\n",
    "\n",
    "with gr.Blocks(title=\"ColPali RAG Pipeline\") as demo:\n",
    "    gr.Markdown(\"## ColPali RAG Pipeline\")\n",
    "\n",
    "    with gr.Row():\n",
    "        pdf_input = gr.File(label=\"Upload PDF\")\n",
    "        query_input = gr.Textbox(label=\"Enter your query\")\n",
    "        retrieve_btn = gr.Button(\"Retrieve\")\n",
    "\n",
    "    retrieved_texts = gr.Textbox(label=\"Top 3 Relevant Texts\", lines=8)\n",
    "    retrieved_images = gr.Gallery(label=\"Top 3 Relevant Pages\", show_label=True)\n",
    "\n",
    "    def retrieve_pipeline(pdf_file, query):\n",
    "        global embedded_docs\n",
    "        if not pdf_file or not query:\n",
    "            return \"Missing input\", []\n",
    "        text_chunks, page_images = extract_chunks_and_images(pdf_file.name)\n",
    "        global image_cache\n",
    "        image_cache = {i: img for i, img in enumerate(page_images)}\n",
    "        captions = generate_image_captions(page_images)\n",
    "        embedded_docs = embed_and_push_to_vespa(text_chunks, page_images, captions, vespa_endpoint)\n",
    "        text_hits, image_hits = maxsim_search(query, embedded_docs, top_k_text=3, top_k_img=3)\n",
    "        save_for_generation(query, text_hits, image_hits)\n",
    "        texts = [desc for _, desc, _ in text_hits]\n",
    "        imgs = [(image_cache[page], f\"Page {page}\") for _, _, page in image_hits if page in image_cache]\n",
    "        return \"\\n---\\n\".join(texts), imgs\n",
    "\n",
    "    retrieve_btn.click(\n",
    "        retrieve_pipeline,\n",
    "        inputs=[pdf_input, query_input],\n",
    "        outputs=[retrieved_texts, retrieved_images]\n",
    "    )\n",
    "\n",
    "    demo.launch(share=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
